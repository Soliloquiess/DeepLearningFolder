{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNj25n34AJ9S5Doxs0gCS7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soliloquiess/DeepLearningFolder/blob/main/20240615_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%9E%90%EC%97%B0%EC%96%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npirJnzikJi8",
        "outputId": "6d32628e-1ba9-4885-c173-2bf7d117fd92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "# 꼬꼬마 형태소 분석기 객체 생성\n",
        "kkma = Kkma()\n",
        "text = \"아버지가 방에 들어갑니다.\"\n",
        "\n",
        "# 형태소 추출\n",
        "morphs = kkma.morphs(text)\n",
        "print(morphs)\n",
        "\n",
        "# 형태소와 품사 태그 추출\n",
        "pos = kkma.pos(text)\n",
        "print(pos)\n",
        "\n",
        "# 명사만 추출\n",
        "nouns = kkma.nouns(text)\n",
        "print(nouns)\n",
        "\n",
        "# 문장 분리\n",
        "sentences = \"오늘 날씨는 어때요? 내일은 덥다던데.\"\n",
        "s = kkma.sentences(sentences)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InH6hubupp2p",
        "outputId": "c48b1e2b-f52a-4657-a67f-954038afa75e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['아버지', '가', '방', '에', '들어가', 'ㅂ니다', '.']\n",
            "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKM'), ('들어가', 'VV'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n",
            "['아버지', '방']\n",
            "['오늘 날씨는 어 때요?', '내일은 덥다 던데.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "# 코모란 형태소 분석기 객체 생성\n",
        "komoran = Komoran()\n",
        "text = \"아버지가 방에 들어갑니다.\"\n",
        "\n",
        "# 형태소 추출\n",
        "morphs = komoran.morphs(text)\n",
        "print(morphs)\n",
        "\n",
        "# 형태소와 품사 태그 추출\n",
        "pos = komoran.pos(text)\n",
        "print(pos)\n",
        "\n",
        "# 명사만 추출\n",
        "nouns = komoran.nouns(text)\n",
        "print(nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6iqelkeqOZo",
        "outputId": "1efa8b33-243f-4837-cacc-7b229fab4e25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['아버지', '가', '방', '에', '들어가', 'ㅂ니다', '.']\n",
            "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n",
            "['아버지', '방']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt #오픈소스 코리안 텍스트. 트위터에서 만듬.\n",
        "\n",
        "# Okt 형태소 분석기 객체 생성\n",
        "okt = Okt()\n",
        "text = \"아버지가 방에 들어갑니다.\"\n",
        "\n",
        "# 형태소 추출\n",
        "morphs = okt.morphs(text)\n",
        "print(morphs)\n",
        "\n",
        "# 형태소와 품사 태그 추출\n",
        "pos = okt.pos(text)\n",
        "print(pos)\n",
        "\n",
        "# 명사만 추출\n",
        "nouns = okt.nouns(text)\n",
        "print(nouns)\n",
        "\n",
        "# 정규화, 어구 추출\n",
        "text = \"오늘 날씨가 좋아욬ㅋㅋ\"\n",
        "print(okt.normalize(text))\n",
        "print(okt.phrases(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FjVExpYrV4K",
        "outputId": "eabedd9e-effd-466b-bab6-56c35d73fea6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['아버지', '가', '방', '에', '들어갑니다', '.']\n",
            "[('아버지', 'Noun'), ('가', 'Josa'), ('방', 'Noun'), ('에', 'Josa'), ('들어갑니다', 'Verb'), ('.', 'Punctuation')]\n",
            "['아버지', '방']\n",
            "오늘 날씨가 좋아요ㅋㅋ\n",
            "['오늘', '오늘 날씨', '좋아욬', '날씨']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran()\n",
        "text = \"우리 챗봇은 엔엘피를 좋아해.\"\n",
        "pos = komoran.pos(text)\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEzWKSQsr4zb",
        "outputId": "e905ae00-e687-4df1-dba7-ddbc3ba58586"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('우리', 'NP'), ('챗봇은', 'NA'), ('엔', 'NNB'), ('엘', 'NNP'), ('피', 'NNG'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EF'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran()\n",
        "text = \"우리 챗봇은 엔엘피를 좋아해.\"\n",
        "pos = komoran.pos(text)\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrXS9ePvs_k0",
        "outputId": "5d8b57ca-1b30-4da1-8181-758f75ac6633"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('우리', 'NP'), ('챗봇은', 'NA'), ('엔', 'NNB'), ('엘', 'NNP'), ('피', 'NNG'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EF'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran(userdic='./user_dic.tsv')\n",
        "text = \"우리 챗봇은 엔엘피를 좋아해.\"\n",
        "pos = komoran.pos(text)\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztgVZef2tDeR",
        "outputId": "4adeb0d0-8023-452a-dde3-bf5b2b4bbcbf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('우리', 'NP'), ('챗봇은', 'NA'), ('엔엘피', 'NNG'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EF'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "import numpy as np\n",
        "\n",
        "# Komoran 형태소 분석기 객체 생성\n",
        "komoran = Komoran()\n",
        "\n",
        "# 분석할 텍스트\n",
        "text = \"오늘 날씨는 구름이 많아요.\"\n",
        "\n",
        "# 명사만 추출\n",
        "nouns = komoran.nouns(text)\n",
        "print(nouns)\n",
        "\n",
        "# 단어 사전 구축 및 단어별 인덱스 부여\n",
        "dics = {}\n",
        "for word in nouns:\n",
        "    if word not in dics.keys():\n",
        "        dics[word] = len(dics)\n",
        "print(dics)\n",
        "\n",
        "# 원-핫 인코딩\n",
        "nb_classes = len(dics) # 총 단어의 개수\n",
        "targets = list(dics.values()) # 단어 인덱스 리스트\n",
        "one_hot_targets = np.eye(nb_classes)[targets] # 단어 인덱스를 원-핫 벡터로 변환\n",
        "#넘파이의 eye 사용\n",
        "print(one_hot_targets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWxDwMzxxdtg",
        "outputId": "d092d108-1ee4-4f23-d709-009ae64e3f78"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['오늘', '날씨', '구름']\n",
            "{'오늘': 0, '날씨': 1, '구름': 2}\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 방식은 글자가 많아지면? 으아아악 하나 단어 표현하려고 엄청 많아짐."
      ],
      "metadata": {
        "id": "w33PLdZfx2Ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 희소표현과 분산표현"
      ],
      "metadata": {
        "id": "Txs6SAkHyAkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from konlpy.tag import Komoran\n",
        "import time\n",
        "\n",
        "\n",
        "# 네이버 영화 리뷰 데이터 읽어옴\n",
        "def read_review_data(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "        data = data[1:] # header 제거\n",
        "    return data\n",
        "\n",
        "\n",
        "# 측정 시작\n",
        "start = time.time()\n",
        "\n",
        "# 리뷰 파일 읽어오기\n",
        "print('1) 말뭉치 데이터 읽기 시작')\n",
        "review_data = read_review_data('./ratings.txt')\n",
        "print(len(review_data)) # 리뷰 데이터 전체 개수\n",
        "print('1) 말뭉치 데이터 읽기 완료: ', time.time() - start)\n",
        "\n",
        "# 문장단위로 명사만 추출해 학습 입력 데이터로 만듬\n",
        "print('2) 형태소에서 명사만 추출 시작')\n",
        "komoran = Komoran()\n",
        "docs = [komoran.nouns(sentence[1]) for sentence in review_data]\n",
        "print('2) 형태소에서 명사만 추출 완료: ', time.time() - start)\n",
        "\n",
        "# word2vec 모델 학습\n",
        "print('3) word2vec 모델 학습 시작')\n",
        "model = Word2Vec(sentences=docs, vector_size=200, window=4, min_count=2, sg=1)\n",
        "print('3) word2vec 모델 학습 완료: ', time.time() - start)\n",
        "\n",
        "# 모델 저장\n",
        "print('4) 학습된 모델 저장 시작')\n",
        "model.save('nvmc.model')\n",
        "print('4) 학습된 모델 저장 완료: ', time.time() - start)\n",
        "\n",
        "# 학습된 말뭉치 개수, 코퍼스 내 전체 단어 개수\n",
        "print(\"corpus_count : \", model.corpus_count)\n",
        "print(\"corpus_total_words : \", model.corpus_total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MooILsfvx7aD",
        "outputId": "4f5e0259-21b0-462f-ee9f-e05287210ddd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) 말뭉치 데이터 읽기 시작\n",
            "200000\n",
            "1) 말뭉치 데이터 읽기 완료:  0.5413227081298828\n",
            "2) 형태소에서 명사만 추출 시작\n",
            "2) 형태소에서 명사만 추출 완료:  159.49896788597107\n",
            "3) word2vec 모델 학습 시작\n",
            "3) word2vec 모델 학습 완료:  182.4870536327362\n",
            "4) 학습된 모델 저장 시작\n",
            "4) 학습된 모델 저장 완료:  182.5418519973755\n",
            "corpus_count :  200000\n",
            "corpus_total_words :  1076896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 모델 로딩\n",
        "model = Word2Vec.load('nvmc.model')\n",
        "print(\"corpus_total_words : \", model.corpus_total_words)\n",
        "\n",
        "# '사랑'이란 단어로 생성한 단어 임베딩 벡터\n",
        "print('사랑 : ', model.wv['사랑'])\n",
        "\n",
        "# 단어 유사도 계산\n",
        "print(\"일요일 = 월요일\\t\", model.wv.similarity(w1='일요일', w2='월요일'))\n",
        "print(\"안성기 = 배우\\t\", model.wv.similarity(w1='안성기', w2='배우'))\n",
        "print(\"대기업 = 삼성\\t\", model.wv.similarity(w1='대기업', w2='삼성'))\n",
        "print(\"일요일 != 삼성\\t\", model.wv.similarity(w1='일요일', w2='삼성'))\n",
        "print(\"히어로 != 삼성\\t\", model.wv.similarity(w1='히어로', w2='삼성'))\n",
        "\n",
        "# 가장 유사한 단어 추출\n",
        "print(model.wv.most_similar(\"안성기\", topn=5))\n",
        "print(model.wv.most_similar(\"시리즈\", topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka7WNO3R0lRx",
        "outputId": "f611eef8-beac-4df7-d7a3-49510748f6c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus_total_words :  1076896\n",
            "사랑 :  [ 0.10410189 -0.67916846  0.38883907  0.3039324  -0.27063802  0.01529511\n",
            " -0.14602433  0.01405726 -0.03419482  0.55895054 -0.10340866 -0.06824514\n",
            "  0.13533117 -0.02070263 -0.3291146   0.30135724 -0.11974196 -0.12033223\n",
            " -0.4109407  -0.67922294  0.43478376  0.09841649  0.05892608  0.21595073\n",
            " -0.18536997  0.0106932   0.06345518  0.02737847 -0.23363866 -0.2931841\n",
            "  0.1987776   0.22027229  0.16752143 -0.04236096 -0.11707924  0.23738243\n",
            " -0.13488674 -0.050031   -0.34862873 -0.04319356 -0.43165448  0.17942803\n",
            "  0.12745665 -0.41646832  0.4684942   0.45491743 -0.1739011  -0.18599992\n",
            "  0.35035568  0.26918286  0.25528097 -0.0852053   0.04906834 -0.00987678\n",
            "  0.024292   -0.01835721 -0.18876529 -0.4693489  -0.33087716  0.07685832\n",
            "  0.21412624  0.03973101 -0.34794056 -0.04763476 -0.28844792  0.13526992\n",
            " -0.16329663  0.03585573 -0.418312    0.6058034   0.26200497 -0.12761593\n",
            "  0.25311562  0.322619    0.24498796 -0.09263137  0.19535428 -0.42974067\n",
            " -0.2845372  -0.1023849   0.23164265 -0.03881566 -0.21041393  0.36480993\n",
            " -0.16058555 -0.31893173 -0.30335024  0.13650629  0.09919376  0.00084741\n",
            "  0.3380552   0.17252958  0.29592374 -0.0649533   0.26047513 -0.24136655\n",
            "  0.07904228  0.11831851 -0.32026333  0.21629146 -0.35004756  0.3671765\n",
            "  0.33753866  0.35547006 -0.01480367 -0.45701635  0.19391748  0.2904958\n",
            " -0.1375181  -0.18660669 -0.10072894 -0.31204647  0.17372938  0.12315943\n",
            "  0.23633248 -0.59664464  0.01819886 -0.25339255  0.19969676  0.14554353\n",
            "  0.15974143  0.12632653 -0.42025092 -0.37452462 -0.15888005  0.36394984\n",
            " -0.00493477  0.15681429 -0.12062053  0.12430689 -0.11196617  0.3782671\n",
            " -0.04474813  0.23481223  0.00687959  0.05758845  0.1546514   0.07151163\n",
            " -0.15393005  0.3458706   0.08305564  0.03241518 -0.5038848  -0.02586938\n",
            "  0.17498574  0.10425008 -0.18588553 -0.08679124 -0.01314871 -0.39100775\n",
            " -0.11310283 -0.5797548   0.27999547 -0.08100703 -0.3243955  -0.11662103\n",
            "  0.35146117  0.07653604  0.39745408  0.1644293   0.14478694  0.3576534\n",
            " -0.07209501 -0.05407458  0.25078493  0.3987722  -0.20460291 -0.45926514\n",
            " -0.22638105  0.01286274 -0.4360934   0.02962078  0.19520044 -0.41414475\n",
            "  0.06004638 -0.32550597  0.42102134 -0.20517498  0.1821435   0.28012714\n",
            " -0.11180145  0.11408123 -0.06429287  0.12841882 -0.29383537 -0.17080215\n",
            "  0.00567022 -0.2855652   0.31293285 -0.09406482  0.20418651 -0.35020608\n",
            " -0.28917983 -0.16908821  0.29253903 -0.01906933  0.11466978 -0.05606367\n",
            "  0.0400026   0.05947614]\n",
            "일요일 = 월요일\t 0.9036973\n",
            "안성기 = 배우\t 0.71998644\n",
            "대기업 = 삼성\t 0.8539032\n",
            "일요일 != 삼성\t 0.60792595\n",
            "히어로 != 삼성\t 0.47845003\n",
            "[('최민식', 0.9394783973693848), ('정재영', 0.9375818371772766), ('박신양', 0.9343463778495789), ('능청', 0.9329574704170227), ('최강희', 0.9320276379585266)]\n",
            "[('엑스맨', 0.8074938058853149), ('해리', 0.7866867780685425), ('미이라', 0.7819952964782715), ('다이하드', 0.7805292010307312), ('마블', 0.7763729095458984)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 모듈 임포트\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
        "\n",
        "# 데이터 읽어오기\n",
        "train_file = \"./chatbot_data.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "MAX_SEQ_LEN = 15  # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "# 학습용, 검증용, 테스트용 데이터셋 생성 ➌\n",
        "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "train_size = int(len(padded_seqs) * 0.7)\n",
        "val_size = int(len(padded_seqs) * 0.2)\n",
        "test_size = int(len(padded_seqs) * 0.1)\n",
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
        "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "dropout_prob = 0.5\n",
        "EMB_SIZE = 128\n",
        "EPOCH = 5\n",
        "VOCAB_SIZE = len(word_index) + 1  # 전체 단어 수\n",
        "\n",
        "# CNN 모델 정의\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
        "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
        "\n",
        "conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "pool1 = GlobalMaxPool1D()(conv1)\n",
        "conv2 = Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "pool2 = GlobalMaxPool1D()(conv2)\n",
        "conv3 = Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
        "pool3 = GlobalMaxPool1D()(conv3)\n",
        "\n",
        "# 3, 4, 5- gram 이후 합치기\n",
        "concat = concatenate([pool1, pool2, pool3])\n",
        "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
        "logits = Dense(3, name='logits')(dropout_hidden)\n",
        "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
        "\n",
        "# 모델 생성\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
        "\n",
        "# 모델 평가(테스트 데이터셋 이용)\n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy * 100))\n",
        "print('loss: %f' % (loss))\n",
        "\n",
        "# 모델 저장\n",
        "model.save('cnn_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmZjfpjp2trI",
        "outputId": "8d1ec335-3e31-4419-e278-dbf5afc92eee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "414/414 [==============================] - 20s 32ms/step - loss: 0.9127 - accuracy: 0.5667 - val_loss: 0.5997 - val_accuracy: 0.7982\n",
            "Epoch 2/5\n",
            "414/414 [==============================] - 3s 6ms/step - loss: 0.5206 - accuracy: 0.8074 - val_loss: 0.2859 - val_accuracy: 0.9061\n",
            "Epoch 3/5\n",
            "414/414 [==============================] - 3s 7ms/step - loss: 0.3146 - accuracy: 0.8948 - val_loss: 0.1829 - val_accuracy: 0.9442\n",
            "Epoch 4/5\n",
            "414/414 [==============================] - 3s 8ms/step - loss: 0.2105 - accuracy: 0.9362 - val_loss: 0.0981 - val_accuracy: 0.9734\n",
            "Epoch 5/5\n",
            "414/414 [==============================] - 2s 6ms/step - loss: 0.1288 - accuracy: 0.9602 - val_loss: 0.0554 - val_accuracy: 0.9822\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.9805\n",
            "Accuracy: 98.054147\n",
            "loss: 0.066035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 모듈 임포트\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
        "\n",
        "# 데이터 읽어오기\n",
        "train_file = \"./chatbot_data.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')  # CSV 파일을 읽어와 DataFrame으로 저장\n",
        "features = data['Q'].tolist()  # 질문 열을 리스트로 변환\n",
        "labels = data['label'].tolist()  # 라벨 열을 리스트로 변환\n",
        "\n",
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]  # 각 질문을 단어 시퀀스로 변환\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()  # 텍스트 토크나이저 객체 생성\n",
        "tokenizer.fit_on_texts(corpus)  # 토크나이저에 말뭉치(corpus) 학습\n",
        "sequences = tokenizer.texts_to_sequences(corpus)  # 말뭉치를 시퀀스로 변환\n",
        "word_index = tokenizer.word_index  # 단어 인덱스 딕셔너리 생성\n",
        "MAX_SEQ_LEN = 15  # 단어 시퀀스 벡터 크기 설정\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')  # 시퀀스를 패딩하여 동일한 길이로 맞춤\n",
        "\n",
        "# 학습용, 검증용, 테스트용 데이터셋 생성\n",
        "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))  # 패딩된 시퀀스와 라벨로 텐서플로 데이터셋 생성\n",
        "ds = ds.shuffle(len(features))  # 데이터셋을 셔플\n",
        "train_size = int(len(padded_seqs) * 0.7)  # 학습 데이터셋 크기 설정\n",
        "val_size = int(len(padded_seqs) * 0.2)  # 검증 데이터셋 크기 설정\n",
        "test_size = int(len(padded_seqs) * 0.1)  # 테스트 데이터셋 크기 설정\n",
        "train_ds = ds.take(train_size).batch(20)  # 학습 데이터셋 생성 및 배치 크기 설정\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)  # 검증 데이터셋 생성 및 배치 크기 설정\n",
        "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)  # 테스트 데이터셋 생성 및 배치 크기 설정\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "dropout_prob = 0.5  # 드롭아웃 확률 설정\n",
        "EMB_SIZE = 128  # 임베딩 벡터 크기 설정\n",
        "EPOCH = 5  # 에포크 수 설정\n",
        "VOCAB_SIZE = len(word_index) + 1  # 전체 단어 수 설정 (단어 인덱스 개수 + 1)\n",
        "\n",
        "# CNN 모델 정의\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN,))  # 입력층 정의, 입력 크기는 시퀀스 길이\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)  # 임베딩 층 정의 및 연결\n",
        "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)  # 임베딩 층에 드롭아웃 적용\n",
        "\n",
        "# 1D 합성곱 및 맥스 풀링 레이어 정의\n",
        "conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)  # 필터 크기 3인 1D 합성곱 레이어\n",
        "pool1 = GlobalMaxPool1D()(conv1)  # 맥스 풀링 레이어\n",
        "conv2 = Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)  # 필터 크기 4인 1D 합성곱 레이어\n",
        "pool2 = GlobalMaxPool1D()(conv2)  # 맥스 풀링 레이어\n",
        "conv3 = Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)  # 필터 크기 5인 1D 합성곱 레이어\n",
        "pool3 = GlobalMaxPool1D()(conv3)  # 맥스 풀링 레이어\n",
        "\n",
        "# 3, 4, 5-gram 이후 합치기\n",
        "concat = concatenate([pool1, pool2, pool3])  # 풀링 레이어를 연결하여 합침\n",
        "hidden = Dense(128, activation=tf.nn.relu)(concat)  # 완전 연결층 정의 및 연결\n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden)  # 완전 연결층에 드롭아웃 적용\n",
        "logits = Dense(3, name='logits')(dropout_hidden)  # 로짓(logits) 출력층 정의 및 연결\n",
        "predictions = Dense(3, activation=tf.nn.softmax)(logits)  # 소프트맥스 출력층 정의 및 연결\n",
        "\n",
        "# 모델 생성\n",
        "model = Model(inputs=input_layer, outputs=predictions)  # 모델 객체 생성\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # 모델 컴파일 (최적화 함수, 손실 함수, 평가 지표 설정)\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)  # 모델 학습 수행\n",
        "\n",
        "# 모델 평가(테스트 데이터셋 이용)\n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1)  # 테스트 데이터셋을 이용한 모델 평가\n",
        "print('Accuracy: %f' % (accuracy * 100))  # 정확도 출력\n",
        "print('loss: %f' % (loss))  # 손실 값 출력\n",
        "\n",
        "# 모델 저장\n",
        "model.save('cnn_model.h5')  # 모델을 파일로 저장\n",
        "#위와 동일한 코드에 주석 추가\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guI3z0Gg5vIE",
        "outputId": "00f6655f-cacb-4550-89b0-24b5062744c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "414/414 [==============================] - 16s 22ms/step - loss: 0.8849 - accuracy: 0.5836 - val_loss: 0.5299 - val_accuracy: 0.8160\n",
            "Epoch 2/5\n",
            "414/414 [==============================] - 3s 6ms/step - loss: 0.5074 - accuracy: 0.8092 - val_loss: 0.2672 - val_accuracy: 0.9196\n",
            "Epoch 3/5\n",
            "414/414 [==============================] - 3s 6ms/step - loss: 0.2912 - accuracy: 0.9015 - val_loss: 0.1371 - val_accuracy: 0.9556\n",
            "Epoch 4/5\n",
            "414/414 [==============================] - 3s 8ms/step - loss: 0.1933 - accuracy: 0.9374 - val_loss: 0.0900 - val_accuracy: 0.9712\n",
            "Epoch 5/5\n",
            "414/414 [==============================] - 3s 7ms/step - loss: 0.1319 - accuracy: 0.9604 - val_loss: 0.0669 - val_accuracy: 0.9784\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0757 - accuracy: 0.9797\n",
            "Accuracy: 97.969544\n",
            "loss: 0.075693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "# 데이터 읽어오기\n",
        "train_file = \"./chatbot_data.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')  # CSV 파일을 읽어와 DataFrame으로 저장\n",
        "features = data['Q'].tolist()  # 질문 열을 리스트로 변환\n",
        "labels = data['label'].tolist()  # 라벨 열을 리스트로 변환\n",
        "\n",
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]  # 각 질문을 단어 시퀀스로 변환\n",
        "tokenizer = preprocessing.text.Tokenizer()  # 텍스트 토크나이저 객체 생성\n",
        "tokenizer.fit_on_texts(corpus)  # 토크나이저에 말뭉치(corpus) 학습\n",
        "sequences = tokenizer.texts_to_sequences(corpus)  # 말뭉치를 시퀀스로 변환\n",
        "MAX_SEQ_LEN = 15  # 단어 시퀀스 벡터 크기 설정\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')  # 시퀀스를 패딩하여 동일한 길이로 맞춤\n",
        "\n",
        "# 테스트용 데이터셋 생성\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))  # 패딩된 시퀀스와 라벨로 텐서플로 데이터셋 생성\n",
        "ds = ds.shuffle(len(features))  # 데이터셋을 셔플\n",
        "test_ds = ds.take(2000).batch(20)  # 테스트 데이터셋 생성 및 배치 크기 설정\n",
        "\n",
        "# 감정 분류 CNN 모델 불러오기\n",
        "model = load_model('cnn_model.h5')  # 저장된 모델 불러오기\n",
        "model.summary()  # 모델 요약 정보 출력\n",
        "model.evaluate(test_ds, verbose=2)  # 테스트 데이터셋을 이용한 모델 평가\n",
        "\n",
        "# 테스트용 데이터셋의 10212번째 데이터 출력\n",
        "print(\"단어 시퀀스 : \", corpus[10212])  # 10212번째 데이터의 단어 시퀀스 출력\n",
        "print(\"단어 인덱스 시퀀스 : \", padded_seqs[10212])  # 10212번째 데이터의 단어 인덱스 시퀀스 출력\n",
        "print(\"문장 분류(정답) : \", labels[10212])  # 10212번째 데이터의 정답 라벨 출력\n",
        "\n",
        "# 테스트용 데이터셋의 10212번째 데이터 감정 예측\n",
        "picks = [10212]  # 예측할 데이터 인덱스 설정\n",
        "predict = model.predict(padded_seqs[picks])  # 모델을 이용한 예측 수행\n",
        "predict_class = tf.math.argmax(predict, axis=1)  # 예측 결과에서 가장 높은 확률의 클래스 인덱스 추출\n",
        "print(\"감정 예측 점수 : \", predict)  # 예측 점수 출력\n",
        "print(\"감정 예측 클래스 : \", predict_class.numpy())  # 예측 클래스 출력\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1yf_3Qs5wep",
        "outputId": "e9e740d2-2394-498e-bf64-4fead423b704"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 15)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 15, 128)              1715072   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 15, 128)              0         ['embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 13, 128)              49280     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)           (None, 12, 128)              65664     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)           (None, 11, 128)              82048     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Gl  (None, 128)                  0         ['conv1d_3[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Gl  (None, 128)                  0         ['conv1d_4[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " global_max_pooling1d_5 (Gl  (None, 128)                  0         ['conv1d_5[0][0]']            \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 384)                  0         ['global_max_pooling1d_3[0][0]\n",
            " )                                                                  ',                            \n",
            "                                                                     'global_max_pooling1d_4[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'global_max_pooling1d_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 128)                  49280     ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 128)                  0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " logits (Dense)              (None, 3)                    387       ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 3)                    12        ['logits[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1961743 (7.48 MB)\n",
            "Trainable params: 1961743 (7.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "100/100 - 0s - loss: 0.0704 - accuracy: 0.9805 - 414ms/epoch - 4ms/step\n",
            "단어 시퀀스 :  ['썸', '타는', '여자가', '남사친', '만나러', '간다는데', '뭐라', '해']\n",
            "단어 인덱스 시퀀스 :  [   13    61   127  4320  1333 12162   856    31     0     0     0     0\n",
            "     0     0     0]\n",
            "문장 분류(정답) :  2\n",
            "1/1 [==============================] - 0s 317ms/step\n",
            "감정 예측 점수 :  [[8.0958876e-07 4.7968649e-07 9.9999869e-01]]\n",
            "감정 예측 클래스 :  [2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 학습 파일 불러오기\n",
        "def read_file(file_name):\n",
        "    sents = []\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for idx, l in enumerate(lines):\n",
        "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
        "                this_sent = []\n",
        "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
        "                continue\n",
        "            elif l[0] == '\\n':\n",
        "                sents.append(this_sent)\n",
        "            else:\n",
        "                this_sent.append(tuple(l.split()))\n",
        "    return sents\n",
        "\n",
        "\n",
        "# 학습용 말뭉치 데이터를 불러옴\n",
        "corpus = read_file('train.txt')\n",
        "\n",
        "# 말뭉치 데이터에서 단어와 BIO 태그만 불러와 학습용 데이터셋 생성\n",
        "sentences, tags = [], []\n",
        "for t in corpus:\n",
        "    tagged_sentence = []\n",
        "    sentence, bio_tag = [], []\n",
        "    for w in t:\n",
        "        tagged_sentence.append((w[1], w[3]))\n",
        "        sentence.append(w[1])\n",
        "        bio_tag.append(w[3])\n",
        "\n",
        "    sentences.append(sentence)\n",
        "    tags.append(bio_tag)\n",
        "\n",
        "print(\"샘플 크기 : \\n\", len(sentences))\n",
        "print(\"0번째 샘플 문장 시퀀스 : \\n\", sentences[0])\n",
        "print(\"0번째 샘플 bio 태그 : \\n\", tags[0])\n",
        "print(\"샘플 문장 시퀀스 최대 길이 :\", max(len(l) for l in sentences))\n",
        "print(\"샘플 문장 시퀀스 평균 길이 :\", (sum(map(len, sentences))/len(sentences)))\n",
        "\n",
        "\n",
        "# 토크나이저 정의\n",
        "sent_tokenizer = preprocessing.text.Tokenizer(oov_token='OOV') # 첫 번째 인덱스에는 OOV 사용\n",
        "sent_tokenizer.fit_on_texts(sentences)\n",
        "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower= False 소문자로 변환하지 않는다.\n",
        "tag_tokenizer.fit_on_texts(tags)\n",
        "\n",
        "# 단어 사전 및 태그 사전 크기\n",
        "vocab_size = len(sent_tokenizer.word_index) + 1\n",
        "tag_size = len(tag_tokenizer.word_index) + 1\n",
        "print(\"BIO 태그 사전 크기 :\", tag_size)\n",
        "print(\"단어 사전 크기 :\", vocab_size)\n",
        "\n",
        "# 학습용 단어 시퀀스 생성\n",
        "x_train = sent_tokenizer.texts_to_sequences(sentences)\n",
        "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
        "print(x_train[0])\n",
        "print(y_train[0])\n",
        "\n",
        "# index to word / index to NER 정의\n",
        "index_to_word = sent_tokenizer.index_word # 시퀀스 인덱스를 단어로 변환하기 위해 사용\n",
        "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환하기 위해 사용\n",
        "index_to_ner[0] = 'PAD'\n",
        "\n",
        "# 시퀀스 패딩 처리\n",
        "max_len = 40\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
        "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터를 8:2 비율로 분리\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=.2, random_state=0)\n",
        "\n",
        "# 출력 데이터를 원-핫 인코딩\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
        "\n",
        "print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
        "print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
        "print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
        "print(\"테스트 샘플 레이블 형상 : \", y_test.shape)\n",
        "\n",
        "# 모델 정의(Bi-LSTM)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
        "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
        "print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])\n",
        "\n",
        "\n",
        "# 시퀀스를 NER 태그로 변환\n",
        "def sequences_to_tag(sequences):\n",
        "    result = []\n",
        "    for sequence in sequences:\n",
        "        temp = []\n",
        "        for pred in sequence:\n",
        "            pred_index = np.argmax(pred)\n",
        "            temp.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\n",
        "        result.append(temp)\n",
        "    return result\n",
        "\n",
        "\n",
        "# 테스트 데이터셋의 NER 예측\n",
        "y_predicted = model.predict(x_test) # (711, 40) => model => (711, 40, 8)\n",
        "pred_tags = sequences_to_tag(y_predicted) # 예측된 NER\n",
        "test_tags = sequences_to_tag(y_test) # 실제 NER\n",
        "\n",
        "# F1 스코어 계산을 위해 사용\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "print(classification_report(test_tags, pred_tags))\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
        "\n",
        "\n",
        "# 새로운 유형의 문장 NER 예측\n",
        "word_to_index = sent_tokenizer.word_index\n",
        "new_sentence = '삼성전자 출시 스마트폰 오늘 애플 도전장 내밀다.'.split()\n",
        "new_x = []\n",
        "for w in new_sentence:\n",
        "    try:\n",
        "        new_x.append(word_to_index.get(w, 1))\n",
        "    except KeyError:\n",
        "        # 모르는 단어의 경우 OOV\n",
        "        new_x.append(word_to_index['OOV'])\n",
        "\n",
        "print(\"새로운 유형의 시퀀스 : \", new_x)\n",
        "new_padded_seqs = preprocessing.sequence.pad_sequences([new_x], padding=\"post\", value=0, maxlen=max_len)\n",
        "\n",
        "# NER 예측\n",
        "p = model.predict(np.array([new_padded_seqs[0]]))\n",
        "p = np.argmax(p, axis=-1) # 예측된 NER 인덱스값 추출\n",
        "print(\"{:10} {:5}\".format(\"단어\", \"예측된 NER\"))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for w, pred in zip(new_sentence, p[0]):\n",
        "    print(\"{:10} {:5}\".format(w, index_to_ner[pred]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTSWcxJlA6rC",
        "outputId": "2d12bcbc-aabd-4ff7-dc80-71d2b1baf3e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플 크기 : \n",
            " 3555\n",
            "0번째 샘플 문장 시퀀스 : \n",
            " ['한편', ',', 'AFC', '챔피언스', '리그', 'E', '조', '에', '속하', 'ㄴ', '포항', '역시', '대회', '8강', '진출', '이', '불투명', '하', '다', '.']\n",
            "0번째 샘플 bio 태그 : \n",
            " ['O', 'O', 'O', 'O', 'O', 'B_OG', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "샘플 문장 시퀀스 최대 길이 : 168\n",
            "샘플 문장 시퀀스 평균 길이 : 34.03909985935302\n",
            "BIO 태그 사전 크기 : 8\n",
            "단어 사전 크기 : 13834\n",
            "[183, 11, 4276, 884, 162, 931, 402, 10, 2608, 7, 1516, 608, 145, 1361, 414, 4, 6347, 2, 8, 3]\n",
            "[1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "학습 샘플 시퀀스 형상 :  (2844, 40)\n",
            "학습 샘플 레이블 형상 :  (2844, 40, 8)\n",
            "테스트 샘플 시퀀스 형상 :  (711, 40)\n",
            "테스트 샘플 레이블 형상 :  (711, 40, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "17/23 [=====================>........] - ETA: 3s - loss: 0.7751 - accuracy: 0.8255"
          ]
        }
      ]
    }
  ]
}